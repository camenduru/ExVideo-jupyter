{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone -b dev https://github.com/camenduru/DiffSynth-Studio\n",
    "%cd /content/DiffSynth-Studio\n",
    "\n",
    "!pip install -q einops transformers controlnet-aux==0.0.7 sentencepiece imageio imageio-ffmpeg\n",
    "\n",
    "!sudo apt -y install -qq aria2\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/resolve/main/t2i/clip_text_encoder/pytorch_model.bin -d /content/DiffSynth-Studio/models/HunyuanDiT/t2i/clip_text_encoder -o pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/resolve/main/t2i/mt5/pytorch_model.bin -d /content/DiffSynth-Studio/models/HunyuanDiT/t2i/mt5 -o pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/resolve/main/t2i/model/pytorch_model_ema.pt -d /content/DiffSynth-Studio/models/HunyuanDiT/t2i/model -o pytorch_model_ema.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/resolve/main/t2i/sdxl-vae-fp16-fix/diffusion_pytorch_model.bin -d /content/DiffSynth-Studio/models/HunyuanDiT/t2i/sdxl-vae-fp16-fix -o diffusion_pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/resolve/main/svd_xt.safetensors -d /content/DiffSynth-Studio/models/stable_video_diffusion -o svd_xt.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1/resolve/main/model.fp16.safetensors -d /content/DiffSynth-Studio/models/stable_video_diffusion -o model.fp16.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/DiffSynth-Studio\n",
    "\n",
    "from diffsynth import save_video, ModelManager, SVDVideoPipeline, HunyuanDiTImagePipeline\n",
    "from diffsynth import ModelManager\n",
    "import torch, os, imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def read_video(file_name):\n",
    "    reader = imageio.get_reader(file_name)\n",
    "    video = []\n",
    "    for frame in reader:\n",
    "        frame = np.array(frame).copy()\n",
    "        video.append(frame)\n",
    "    reader.close()\n",
    "    return video\n",
    "\n",
    "def generate_image():\n",
    "    # Load models\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "    model_manager = ModelManager(torch_dtype=torch.float16, device=\"cuda\")\n",
    "    model_manager.load_models([\n",
    "        \"models/HunyuanDiT/t2i/clip_text_encoder/pytorch_model.bin\",\n",
    "        \"models/HunyuanDiT/t2i/mt5/pytorch_model.bin\",\n",
    "        \"models/HunyuanDiT/t2i/model/pytorch_model_ema.pt\",\n",
    "        \"models/HunyuanDiT/t2i/sdxl-vae-fp16-fix/diffusion_pytorch_model.bin\"\n",
    "    ])\n",
    "    pipe = HunyuanDiTImagePipeline.from_model_manager(model_manager)\n",
    "\n",
    "    # Generate an image\n",
    "    torch.manual_seed(0)\n",
    "    image = pipe(\n",
    "        prompt=\"bonfire, on the stone\",\n",
    "        negative_prompt=\"错误的眼睛，糟糕的人脸，毁容，糟糕的艺术，变形，多余的肢体，模糊的颜色，模糊，重复，病态，残缺，\",\n",
    "        num_inference_steps=50, height=1024, width=1024,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def generate_video(image):\n",
    "    # Load models\n",
    "    model_manager = ModelManager(torch_dtype=torch.float16, device=\"cuda\")\n",
    "    model_manager.load_models([\n",
    "        \"models/stable_video_diffusion/svd_xt.safetensors\",\n",
    "        \"models/stable_video_diffusion/model.fp16.safetensors\"\n",
    "    ])\n",
    "    pipe = SVDVideoPipeline.from_model_manager(model_manager)\n",
    "\n",
    "    # Generate a video\n",
    "    torch.manual_seed(1)\n",
    "    video = pipe(\n",
    "        input_image=image.resize((512, 512)),\n",
    "        num_frames=128, fps=30, height=512, width=512,\n",
    "        motion_bucket_id=127,\n",
    "        num_inference_steps=50,\n",
    "        min_cfg_scale=2, max_cfg_scale=2, contrast_enhance_scale=1.2\n",
    "    )\n",
    "    return video\n",
    "\n",
    "\n",
    "def upscale_video(image, video):\n",
    "    # Load models\n",
    "    model_manager = ModelManager(torch_dtype=torch.float16, device=\"cuda\")\n",
    "    model_manager.load_models([\n",
    "        \"models/stable_video_diffusion/svd_xt.safetensors\",\n",
    "        \"models/stable_video_diffusion/model.fp16.safetensors\",\n",
    "    ])\n",
    "    pipe = SVDVideoPipeline.from_model_manager(model_manager)\n",
    "\n",
    "    pil_frames = [Image.fromarray(frame) for frame in video]\n",
    "    resized_frames = [frame.resize((1024, 1024)) for frame in pil_frames]\n",
    "\n",
    "    # Generate a video\n",
    "    torch.manual_seed(2)\n",
    "    video = pipe(\n",
    "        input_image=image.resize((1024, 1024)),\n",
    "        input_video=resized_frames,\n",
    "        num_frames=128, fps=30, height=1024, width=1024,\n",
    "        motion_bucket_id=127,\n",
    "        num_inference_steps=25,\n",
    "        min_cfg_scale=2, max_cfg_scale=2, contrast_enhance_scale=1.2\n",
    "    )\n",
    "    return video\n",
    "\n",
    "\n",
    "# We use Hunyuan DiT to generate the first frame.\n",
    "# If you want to use your own image,\n",
    "# please use `image = Image.open(\"your_image_file.png\")` to replace the following code.\n",
    "image = generate_image()\n",
    "image.save(\"image.png\")\n",
    "image = Image.open(\"/content/DiffSynth-Studio/image.png\")\n",
    "\n",
    "# Now, generate a video with resolution of 512.\n",
    "video = generate_video(image)\n",
    "save_video(video, \"video_512.mp4\", fps=30)\n",
    "\n",
    "# Upscale the video.\n",
    "# video = upscale_video(image, video)\n",
    "# save_video(video, \"video_1024.mp4\", fps=30)\n",
    "\n",
    "image = Image.open(\"/content/DiffSynth-Studio/image.png\")\n",
    "video = read_video(\"/content/DiffSynth-Studio/video_512.mp4\")\n",
    "video = upscale_video(image, video)\n",
    "save_video(video, \"video_1024.mp4\", fps=30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
